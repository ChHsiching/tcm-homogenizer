#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è’™ç‰¹å¡ç½—åˆ†æç®—æ³•å®ç°
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from loguru import logger
import json
from pathlib import Path
import time
import random

class MonteCarloAnalysis:
    """è’™ç‰¹å¡ç½—åˆ†æç®—æ³•"""
    
    def __init__(self):
        self.results = {}
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        self._load_saved_results()
    
    def analyze(self, data: Dict[str, Any], target_column: str, 
                feature_columns: List[str], iterations: int = 1000) -> Dict[str, Any]:
        """æ‰§è¡Œè’™ç‰¹å¡ç½—åˆ†æ"""
        try:
            logger.info(f"ğŸ² å¼€å§‹è’™ç‰¹å¡ç½—åˆ†æï¼Œç›®æ ‡å˜é‡: {target_column}, ç‰¹å¾å˜é‡: {feature_columns}")
            logger.info(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iterations}")
            
            # æ•°æ®é¢„å¤„ç†
            df = pd.DataFrame(data['data'])
            logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            if target_column not in df.columns:
                raise ValueError(f"ç›®æ ‡å˜é‡åˆ— '{target_column}' ä¸å­˜åœ¨")
            
            for col in feature_columns:
                if col not in df.columns:
                    raise ValueError(f"ç‰¹å¾å˜é‡åˆ— '{col}' ä¸å­˜åœ¨")
            
            # æå–æ•°æ®
            X = df[feature_columns].values
            y = df[target_column].values
            
            logger.info(f"ğŸ“Š ç‰¹å¾æ•°æ®å½¢çŠ¶: {X.shape}")
            logger.info(f"ğŸ“Š ç›®æ ‡æ•°æ®å½¢çŠ¶: {y.shape}")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if len(y) < 10:
                raise ValueError("æ•°æ®æ ·æœ¬æ•°é‡å¤ªå°‘ï¼Œè‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬")
            
            if y.std() < 1e-6:
                raise ValueError("ç›®æ ‡å˜é‡æ²¡æœ‰å˜åŒ–ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            
            # æ£€æŸ¥NaNå€¼
            if np.isnan(X).any() or np.isnan(y).any():
                raise ValueError("æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
            
            # æ‰§è¡Œè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ
            logger.info("ğŸ”„ å¼€å§‹è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ...")
            
            # è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            target_mean = np.mean(y)
            target_std = np.std(y)
            feature_means = np.mean(X, axis=0)
            feature_stds = np.std(X, axis=0)
            
            logger.info(f"ğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡: å‡å€¼={target_mean:.3f}, æ ‡å‡†å·®={target_std:.3f}")
            
            # è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ
            simulations = []
            for i in range(iterations):
                # éšæœºç”Ÿæˆç‰¹å¾å€¼
                simulated_features = np.random.normal(feature_means, feature_stds)
                
                # è®¡ç®—æ¨¡æ‹Ÿçš„ç›®æ ‡å€¼ï¼ˆåŸºäºçº¿æ€§ç»„åˆï¼‰
                simulated_target = np.sum(simulated_features * np.random.uniform(-1, 1, len(feature_columns)))
                
                # æ·»åŠ å™ªå£°
                simulated_target += np.random.normal(0, target_std * 0.1)
                
                simulations.append({
                    'features': simulated_features.tolist(),
                    'target': simulated_target,
                    'iteration': i + 1
                })
                
                if (i + 1) % 100 == 0:
                    logger.info(f"ğŸ”„ å®Œæˆ {i + 1}/{iterations} æ¬¡æ¨¡æ‹Ÿ")
            
            # åˆ†æç»“æœ
            simulated_targets = [s['target'] for s in simulations]
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            analysis_result = {
                'target_statistics': {
                    'mean': float(np.mean(simulated_targets)),
                    'std': float(np.std(simulated_targets)),
                    'min': float(np.min(simulated_targets)),
                    'max': float(np.max(simulated_targets)),
                    'median': float(np.median(simulated_targets))
                },
                'feature_importance': self._calculate_feature_importance(simulations, feature_columns),
                'confidence_intervals': self._calculate_confidence_intervals(simulated_targets),
                'simulations': simulations[:100],  # åªè¿”å›å‰100ä¸ªæ¨¡æ‹Ÿç»“æœ
                'parameters': {
                    'iterations': iterations,
                    'target_column': target_column,
                    'feature_columns': feature_columns,
                    'n_features': len(feature_columns),
                    'n_samples': len(y)
                },
                'timestamp': time.time()
            }
            
            # ä¿å­˜ç»“æœ
            result_id = int(time.time())
            analysis_result['result_id'] = result_id
            self._save_result(result_id, analysis_result)
            
            logger.info(f"âœ… è’™ç‰¹å¡ç½—åˆ†æå®Œæˆï¼Œç»“æœID: {result_id}")
            logger.info(f"ğŸ“Š æ¨¡æ‹Ÿç›®æ ‡å€¼ç»Ÿè®¡: å‡å€¼={analysis_result['target_statistics']['mean']:.3f}, æ ‡å‡†å·®={analysis_result['target_statistics']['std']:.3f}")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ è’™ç‰¹å¡ç½—åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def _calculate_feature_importance(self, simulations: List[Dict], feature_columns: List[str]) -> List[Dict]:
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            # åŸºäºæ¨¡æ‹Ÿç»“æœè®¡ç®—ç‰¹å¾é‡è¦æ€§
            importance_scores = []
            
            for i, feature in enumerate(feature_columns):
                # è®¡ç®—è¯¥ç‰¹å¾ä¸ç›®æ ‡å€¼çš„ç›¸å…³æ€§
                feature_values = [s['features'][i] for s in simulations]
                target_values = [s['target'] for s in simulations]
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                correlation = np.corrcoef(feature_values, target_values)[0, 1]
                if np.isnan(correlation):
                    correlation = 0
                
                importance_scores.append({
                    'feature': feature,
                    'importance': abs(correlation),
                    'correlation': correlation
                })
            
            # æŒ‰é‡è¦æ€§æ’åº
            importance_scores.sort(key=lambda x: x['importance'], reverse=True)
            
            logger.info(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆï¼Œæœ€é«˜é‡è¦æ€§: {importance_scores[0]['feature']} ({importance_scores[0]['importance']:.3f})")
            
            return importance_scores
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {str(e)}")
            return []
    
    def _calculate_confidence_intervals(self, values: List[float]) -> Dict[str, float]:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        try:
            sorted_values = sorted(values)
            n = len(sorted_values)
            
            # 95%ç½®ä¿¡åŒºé—´
            lower_95 = sorted_values[int(0.025 * n)]
            upper_95 = sorted_values[int(0.975 * n)]
            
            # 90%ç½®ä¿¡åŒºé—´
            lower_90 = sorted_values[int(0.05 * n)]
            upper_90 = sorted_values[int(0.95 * n)]
            
            # 80%ç½®ä¿¡åŒºé—´
            lower_80 = sorted_values[int(0.1 * n)]
            upper_80 = sorted_values[int(0.9 * n)]
            
            return {
                'confidence_95': {'lower': float(lower_95), 'upper': float(upper_95)},
                'confidence_90': {'lower': float(lower_90), 'upper': float(upper_90)},
                'confidence_80': {'lower': float(lower_80), 'upper': float(upper_80)}
            }
            
        except Exception as e:
            logger.error(f"âŒ ç½®ä¿¡åŒºé—´è®¡ç®—å¤±è´¥: {str(e)}")
            return {}
    
    def _save_result(self, result_id: int, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            # ä¿å­˜åˆ°å†…å­˜
            self.results[result_id] = result
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            result_file = self.results_dir / f"{result_id}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
            
        except Exception as e:
            logger.error(f"âŒ ç»“æœä¿å­˜å¤±è´¥: {str(e)}")
            raise
    
    def get_result(self, result_id: str) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æç»“æœ"""
        return self.results.get(result_id)
    
    def get_saved_results(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å·²ä¿å­˜çš„ç»“æœ"""
        results = []
        for result_id, result in self.results.items():
            results.append({
                'result_id': result_id,
                'target_column': result['parameters']['target_column'],
                'iterations': result['parameters']['iterations'],
                'timestamp': result['timestamp']
            })
        return results
    
    def _load_saved_results(self):
        """åŠ è½½å·²ä¿å­˜çš„ç»“æœ"""
        try:
            for result_file in self.results_dir.glob("*.json"):
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                    result_id = result.get('result_id', result_file.stem)
                    self.results[result_id] = result
                except Exception as e:
                    logger.warning(f"âš ï¸  åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥ {result_file}: {str(e)}")
            
            logger.info(f"ğŸ“ å·²åŠ è½½ {len(self.results)} ä¸ªåˆ†æç»“æœ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä¿å­˜çš„ç»“æœå¤±è´¥: {str(e)}") 