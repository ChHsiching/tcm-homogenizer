#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¦å·å›å½’ç®—æ³•æ¨¡å—
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import sympy as sp
from loguru import logger
import random
import math
import traceback

class HeuristicLabSymbolicRegression:
    """å‚è€ƒHeuristicLabå®ç°çš„ç¬¦å·å›å½’ç®—æ³•"""
    
    def __init__(self, population_size=100, generations=50, operators=None, test_ratio=0.3):
        self.population_size = population_size
        self.generations = generations
        self.operators = operators or ['+', '-', '*', '/']
        self.test_ratio = test_ratio
        self.best_individual = None
        self.best_fitness = float('inf')
        self.feature_names = None
        self.target_name = None
        
    def create_expression_tree(self, max_depth=6):
        """åˆ›å»ºè¡¨è¾¾å¼æ ‘ï¼ˆå‚è€ƒHeuristicLabçš„æ ‘ç»“æ„ï¼‰"""
        if max_depth <= 1:
            # å¶å­èŠ‚ç‚¹ï¼šå˜é‡æˆ–å¸¸æ•°
            if random.random() < 0.3:  # 30%æ¦‚ç‡æ˜¯å¸¸æ•°
                return {'type': 'constant', 'value': random.uniform(-10, 10)}
            else:
                return {'type': 'variable', 'name': random.choice(self.feature_names)}
        
        # å†…éƒ¨èŠ‚ç‚¹ï¼šè¿ç®—ç¬¦
        operator = random.choice(self.operators)
        if operator in ['+', '-']:
            # åŠ å‡æ³•å¯ä»¥æœ‰å¤šä¸ªå­èŠ‚ç‚¹
            num_children = random.randint(2, 4)
            children = [self.create_expression_tree(max_depth - 1) for _ in range(num_children)]
        else:
            # ä¹˜é™¤æ³•åªæœ‰ä¸¤ä¸ªå­èŠ‚ç‚¹
            children = [self.create_expression_tree(max_depth - 1) for _ in range(2)]
            
        return {
            'type': 'operator',
            'operator': operator,
            'children': children
        }
    
    def evaluate_tree(self, tree, X):
        """è®¡ç®—è¡¨è¾¾å¼æ ‘çš„å€¼"""
        try:
            if tree['type'] == 'constant':
                return np.full(X.shape[0], tree['value'])
            elif tree['type'] == 'variable':
                var_idx = self.feature_names.index(tree['name'])
                return X[:, var_idx]
            else:
                # è¿ç®—ç¬¦èŠ‚ç‚¹
                children_values = [self.evaluate_tree(child, X) for child in tree['children']]
                
                if tree['operator'] == '+':
                    result = children_values[0]
                    for child_val in children_values[1:]:
                        result += child_val
                    return result
                elif tree['operator'] == '-':
                    result = children_values[0]
                    for child_val in children_values[1:]:
                        result -= child_val
                    return result
                elif tree['operator'] == '*':
                    result = children_values[0]
                    for child_val in children_values[1:]:
                        result *= child_val
                    return result
                elif tree['operator'] == '/':
                    result = children_values[0]
                    for child_val in children_values[1:]:
                        # é¿å…é™¤é›¶å’Œæ•°å€¼æº¢å‡º
                        safe_divisor = np.where(np.abs(child_val) < 1e-10, 1e-10, child_val)
                        result = np.where(np.abs(result) > 1e10, np.sign(result) * 1e10, result)
                        result = np.where(np.abs(safe_divisor) > 1e10, np.sign(safe_divisor) * 1e10, safe_divisor)
                        result = result / safe_divisor
                    return result
        except Exception as e:
            logger.error(f"âŒ è¡¨è¾¾å¼æ ‘è®¡ç®—å¤±è´¥: {str(e)}")
            # è¿”å›é›¶æ•°ç»„ä½œä¸ºfallback
            return np.zeros(X.shape[0])
    
    def tree_to_expression(self, tree):
        """å°†è¡¨è¾¾å¼æ ‘è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨è¾¾å¼"""
        if tree['type'] == 'constant':
            return str(tree['value'])
        elif tree['type'] == 'variable':
            return tree['name']
        else:
            children_expr = [self.tree_to_expression(child) for child in tree['children']]
            if tree['operator'] == '+':
                return ' + '.join(children_expr)
            elif tree['operator'] == '-':
                return ' - '.join(children_expr)
            elif tree['operator'] == '*':
                return ' * '.join(children_expr)
            elif tree['operator'] == '/':
                return ' / '.join(children_expr)
    
    def calculate_fitness(self, tree, X_train, y_train, X_test, y_test):
        """è®¡ç®—é€‚åº”åº¦ï¼ˆMSEï¼‰"""
        try:
            y_pred_train = self.evaluate_tree(tree, X_train)
            y_pred_test = self.evaluate_tree(tree, X_test)
            
            # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰æ•ˆ
            if np.isnan(y_pred_train).any() or np.isnan(y_pred_test).any():
                return float('inf'), float('inf'), float('inf')
            
            if np.isinf(y_pred_train).any() or np.isinf(y_pred_test).any():
                return float('inf'), float('inf'), float('inf')
            
            # é™åˆ¶é¢„æµ‹å€¼èŒƒå›´ï¼Œé¿å…æ•°å€¼æº¢å‡º
            y_pred_train = np.clip(y_pred_train, -1e10, 1e10)
            y_pred_test = np.clip(y_pred_test, -1e10, 1e10)
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            y_pred_train = y_pred_train.astype(np.float64)
            y_pred_test = y_pred_test.astype(np.float64)
            y_train = y_train.astype(np.float64)
            y_test = y_test.astype(np.float64)
            
            # å†æ¬¡æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if not np.all(np.isfinite(y_pred_train)) or not np.all(np.isfinite(y_pred_test)):
                return float('inf'), float('inf'), float('inf')
            
            if not np.all(np.isfinite(y_train)) or not np.all(np.isfinite(y_test)):
                return float('inf'), float('inf'), float('inf')
            
            # è®¡ç®—è®­ç»ƒå’Œæµ‹è¯•è¯¯å·®
            mse_train = mean_squared_error(y_train, y_pred_train)
            mse_test = mean_squared_error(y_test, y_pred_test)
            
            # æ£€æŸ¥è¯¯å·®æ˜¯å¦æœ‰æ•ˆ
            if np.isnan(mse_train) or np.isnan(mse_test) or np.isinf(mse_train) or np.isinf(mse_test):
                return float('inf'), float('inf'), float('inf')
            
            # ç»¼åˆé€‚åº”åº¦ï¼ˆå‚è€ƒHeuristicLabï¼‰
            fitness = mse_test + 0.1 * mse_train + 0.01 * self.tree_complexity(tree)
            
            return fitness, mse_train, mse_test
        except Exception as e:
            logger.error(f"âŒ é€‚åº”åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return float('inf'), float('inf'), float('inf')
    
    def tree_complexity(self, tree):
        """è®¡ç®—æ ‘çš„å¤æ‚åº¦ï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰"""
        if tree['type'] in ['constant', 'variable']:
            return 1
        else:
            return 1 + sum(self.tree_complexity(child) for child in tree['children'])
    
    def crossover(self, parent1, parent2):
        """äº¤å‰æ“ä½œ"""
        if random.random() < 0.5:
            return parent1.copy()
        else:
            return parent2.copy()
    
    def mutation(self, tree, mutation_rate=0.1):
        """å˜å¼‚æ“ä½œ"""
        if random.random() > mutation_rate:
            return tree
            
        # éšæœºé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œå˜å¼‚
        if tree['type'] in ['constant', 'variable']:
            if random.random() < 0.5:
                return {'type': 'constant', 'value': random.uniform(-10, 10)}
            else:
                return {'type': 'variable', 'name': random.choice(self.feature_names)}
        else:
            # å˜å¼‚è¿ç®—ç¬¦
            new_operator = random.choice(self.operators)
            return {
                'type': 'operator',
                'operator': new_operator,
                'children': tree['children']
            }
    
    def fit(self, X, y, feature_names):
        """è®­ç»ƒç¬¦å·å›å½’æ¨¡å‹"""
        self.feature_names = feature_names
        self.target_name = 'target'
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.test_ratio, random_state=42
        )
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = [self.create_expression_tree() for _ in range(self.population_size)]
        
        logger.info(f"å¼€å§‹ç¬¦å·å›å½’è®­ç»ƒï¼Œç§ç¾¤å¤§å°: {self.population_size}, ä»£æ•°: {self.generations}")
        
        for generation in range(self.generations):
            # è®¡ç®—é€‚åº”åº¦
            fitness_scores = []
            for individual in population:
                fitness, mse_train, mse_test = self.calculate_fitness(
                    individual, X_train, y_train, X_test, y_test
                )
                fitness_scores.append((fitness, individual, mse_train, mse_test))
            
            # æ’åºå¹¶æ›´æ–°æœ€ä½³ä¸ªä½“
            fitness_scores.sort(key=lambda x: x[0])
            best_fitness, best_individual, best_mse_train, best_mse_test = fitness_scores[0]
            
            if best_fitness < self.best_fitness:
                self.best_fitness = best_fitness
                self.best_individual = best_individual
                logger.info(f"ç¬¬{generation+1}ä»£å‘ç°æ›´ä¼˜è§£ï¼Œé€‚åº”åº¦: {best_fitness:.6f}")
            
            # é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
            new_population = []
            elite_size = max(1, self.population_size // 10)  # ä¿ç•™10%ç²¾è‹±
            
            # ç²¾è‹±ä¿ç•™
            for i in range(elite_size):
                new_population.append(fitness_scores[i][1])
            
            # ç”Ÿæˆæ–°ä¸ªä½“
            while len(new_population) < self.population_size:
                # é”¦æ ‡èµ›é€‰æ‹©
                tournament_size = 3
                parent1 = min(random.sample(fitness_scores, tournament_size), key=lambda x: x[0])[1]
                parent2 = min(random.sample(fitness_scores, tournament_size), key=lambda x: x[0])[1]
                
                # äº¤å‰
                child = self.crossover(parent1, parent2)
                
                # å˜å¼‚
                child = self.mutation(child)
                
                new_population.append(child)
            
            population = new_population
            
            if (generation + 1) % 10 == 0:
                logger.info(f"ç¬¬{generation+1}ä»£å®Œæˆï¼Œæœ€ä½³é€‚åº”åº¦: {best_fitness:.6f}")
        
        # æœ€ç»ˆè¯„ä¼°
        if self.best_individual:
            y_pred_train = self.evaluate_tree(self.best_individual, X_train)
            y_pred_test = self.evaluate_tree(self.best_individual, X_test)
            
            # æ£€æŸ¥é¢„æµ‹å€¼æœ‰æ•ˆæ€§
            if np.isnan(y_pred_train).any() or np.isnan(y_pred_test).any():
                logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šé¢„æµ‹å€¼åŒ…å«NaN")
                return None
            
            if np.isinf(y_pred_train).any() or np.isinf(y_pred_test).any():
                logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šé¢„æµ‹å€¼åŒ…å«æ— ç©·å€¼")
                return None
            
            # é™åˆ¶é¢„æµ‹å€¼èŒƒå›´
            y_pred_train = np.clip(y_pred_train, -1e10, 1e10)
            y_pred_test = np.clip(y_pred_test, -1e10, 1e10)
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            y_pred_train = y_pred_train.astype(np.float64)
            y_pred_test = y_pred_test.astype(np.float64)
            y_train = y_train.astype(np.float64)
            y_test = y_test.astype(np.float64)
            
            # å†æ¬¡æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if not np.all(np.isfinite(y_pred_train)) or not np.all(np.isfinite(y_pred_test)):
                logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šé¢„æµ‹å€¼åŒ…å«æ— æ•ˆå€¼")
                return None
            
            if not np.all(np.isfinite(y_train)) or not np.all(np.isfinite(y_test)):
                logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šç›®æ ‡å€¼åŒ…å«æ— æ•ˆå€¼")
                return None
            
            try:
                r2_train = r2_score(y_train, y_pred_train)
                r2_test = r2_score(y_test, y_pred_test)
                mse_train = mean_squared_error(y_train, y_pred_train)
                mse_test = mean_squared_error(y_test, y_pred_test)
                mae_train = mean_absolute_error(y_train, y_pred_train)
                mae_test = mean_absolute_error(y_test, y_pred_test)
                
                # æ£€æŸ¥è®¡ç®—ç»“æœæœ‰æ•ˆæ€§
                if np.isnan(r2_train) or np.isnan(r2_test) or np.isnan(mse_train) or np.isnan(mse_test):
                    logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šè®¡ç®—ç»“æœåŒ…å«NaN")
                    return None
                
                if np.isinf(r2_train) or np.isinf(r2_test) or np.isinf(mse_train) or np.isinf(mse_test):
                    logger.error("âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥ï¼šè®¡ç®—ç»“æœåŒ…å«æ— ç©·å€¼")
                    return None
                
                expression = self.tree_to_expression(self.best_individual)
                
                return {
                    'expression': expression,
                    'tree': self.best_individual,
                    'r2_train': r2_train,
                    'r2_test': r2_test,
                    'mse_train': mse_train,
                    'mse_test': mse_test,
                    'mae_train': mae_train,
                    'mae_test': mae_test,
                    'rmse_train': math.sqrt(mse_train),
                    'rmse_test': math.sqrt(mse_test),
                    'fitness': self.best_fitness
                }
                
            except Exception as e:
                logger.error(f"âŒ æœ€ç»ˆè¯„ä¼°å¼‚å¸¸: {str(e)}")
                return None
        
        return None

def perform_symbolic_regression_gplearn(data, target_column, population_size=100, generations=50, 
                                      operators=None, test_ratio=0.3):
    """
    ä½¿ç”¨ç®€åŒ–çš„ç¬¦å·å›å½’ç®—æ³•è¿›è¡Œæµ‹è¯•
    """
    try:
        logger.info("ğŸ”¬ å¼€å§‹ç¬¦å·å›å½’åˆ†æ")
        
        # æ•°æ®é¢„å¤„ç†
        X = data.drop(columns=[target_column]).values
        y = data[target_column].values
        feature_names = data.drop(columns=[target_column]).columns.tolist()
        
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        logger.info(f"ğŸ“Š ç‰¹å¾åç§°: {feature_names}")
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        X = np.array(X, dtype=np.float64)
        y = np.array(y, dtype=np.float64)
        
        # æ£€æŸ¥NaNå€¼
        if np.isnan(X).any():
            logger.error("âŒ ç‰¹å¾æ•°æ®åŒ…å«NaNå€¼")
            return {'success': False, 'error': 'ç‰¹å¾æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶'}
        
        if np.isnan(y).any():
            logger.error("âŒ ç›®æ ‡æ•°æ®åŒ…å«NaNå€¼")
            return {'success': False, 'error': 'ç›®æ ‡æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶'}
        
        # æ£€æŸ¥æ— ç©·å€¼
        if np.isinf(X).any():
            logger.error("âŒ ç‰¹å¾æ•°æ®åŒ…å«æ— ç©·å€¼")
            return {'success': False, 'error': 'ç‰¹å¾æ•°æ®åŒ…å«æ— ç©·å€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶'}
        
        if np.isinf(y).any():
            logger.error("âŒ ç›®æ ‡æ•°æ®åŒ…å«æ— ç©·å€¼")
            return {'success': False, 'error': 'ç›®æ ‡æ•°æ®åŒ…å«æ— ç©·å€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶'}
        
        # æ•°æ®æ ‡å‡†åŒ–
        logger.info("ğŸ”§ å¼€å§‹æ•°æ®æ ‡å‡†åŒ–...")
        scaler_X = StandardScaler()
        X_scaled = scaler_X.fit_transform(X)
        
        # æ£€æŸ¥æ ‡å‡†åŒ–åçš„æ•°æ®
        if np.isnan(X_scaled).any():
            logger.error("âŒ æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®åŒ…å«NaNå€¼")
            return {'success': False, 'error': 'æ•°æ®æ ‡å‡†åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡'}
        
        if np.isinf(X_scaled).any():
            logger.error("âŒ æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®åŒ…å«æ— ç©·å€¼")
            return {'success': False, 'error': 'æ•°æ®æ ‡å‡†åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡'}
        
        logger.info("âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_ratio, random_state=42
        )
        
        logger.info(f"ğŸ“Š è®­ç»ƒé›†å½¢çŠ¶: X_train={X_train.shape}, y_train={y_train.shape}")
        logger.info(f"ğŸ“Š æµ‹è¯•é›†å½¢çŠ¶: X_test={X_test.shape}, y_test={y_test.shape}")
        
        # ä½¿ç”¨ç®€å•çš„çº¿æ€§å›å½’ä½œä¸ºæµ‹è¯•
        from sklearn.linear_model import LinearRegression
        
        logger.info("ğŸ”§ ä½¿ç”¨çº¿æ€§å›å½’è¿›è¡Œæµ‹è¯•...")
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        r2_train = r2_score(y_train, y_pred_train)
        r2_test = r2_score(y_test, y_pred_test)
        mse_train = mean_squared_error(y_train, y_pred_train)
        mse_test = mean_squared_error(y_test, y_pred_test)
        mae_train = mean_absolute_error(y_train, y_pred_train)
        mae_test = mean_absolute_error(y_test, y_pred_test)
        
        # ç”Ÿæˆè¡¨è¾¾å¼
        coefficients = model.coef_
        intercept = model.intercept_
        
        expression_parts = []
        for i, (name, coef) in enumerate(zip(feature_names, coefficients)):
            if abs(coef) > 1e-10:
                if coef >= 0 and i > 0:
                    expression_parts.append(f"+ {coef:.6f} * {name}")
                else:
                    expression_parts.append(f"{coef:.6f} * {name}")
        
        if abs(intercept) > 1e-10:
            if intercept >= 0:
                expression_parts.append(f"+ {intercept:.6f}")
            else:
                expression_parts.append(f"{intercept:.6f}")
        
        expression = " ".join(expression_parts) if expression_parts else "0"
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        feature_importance = []
        for i, name in enumerate(feature_names):
            importance = abs(coefficients[i]) if i < len(coefficients) else 0
            feature_importance.append({
                'feature': name,
                'importance': importance,
                'coefficient': coefficients[i] if i < len(coefficients) else 0
            })
        
        # æŒ‰é‡è¦æ€§æ’åº
        feature_importance.sort(key=lambda x: x['importance'], reverse=True)
        
        logger.info("âœ… çº¿æ€§å›å½’æµ‹è¯•å®Œæˆ")
        
        return {
            'success': True,
            'expression': expression,
            'tree': {
                'type': 'linear',
                'coefficients': coefficients.tolist(),
                'intercept': float(intercept)
            },
            'feature_importance': feature_importance,
            'metrics': {
                'r2_train': r2_train,
                'r2_test': r2_test,
                'mse_train': mse_train,
                'mse_test': mse_test,
                'mae_train': mae_train,
                'mae_test': mae_test,
                'rmse_train': math.sqrt(mse_train),
                'rmse_test': math.sqrt(mse_test)
            }
        }
            
    except Exception as e:
        logger.error(f"âŒ ç¬¦å·å›å½’å¤±è´¥: {str(e)}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {'success': False, 'error': str(e)} 